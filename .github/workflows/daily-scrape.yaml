name: Scrape Data

on:
  push:
    branches:
      - main
  schedule:
    - cron: '0 0 * * *' # This will run the action daily at midnight

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 splinter webdriver_manager

    - name: Install Google Chrome
      run: |
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Run scraping script
      run: python scrape.py

    - name: Commit and push data
      run: |
        git config --global user.email "you@example.com"
        git config --global user.name "Your Name"
        git add docs/data/data.json
        git commit -m "Update scraped data"
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
